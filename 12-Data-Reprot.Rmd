# Model Cross Validation


jeff Leek

   
>
>"...with four parameters I can fit an elephant, and with five I can make him wiggle his trunk."
>
>                                        -- John von Neumann
>
>
>You are entitled to your own opinion, but you are not entitled to your own facts.
>
>
>                                           --   Daniel Patrick Moynihan

In the previous two chapters we have build a number of prediction models using both decision tree and random forest, the two very popular prediction models. Our model have produced different prediction accuracy. The big problem with all the models is that they all have a reduced prediction accuracy with the test dataset. The worse thing is that the reduction of the prediction accuracy on each model is quid different. Together they make us don't know which model should be used for real prediction to achieve the best possible result. 

We are luck because we have Kaggle competition that provides us with a test dataset and the feedback of our model's performance on the test dataset. In real applications, as the titanic competition simulated the test dataset has no response variables (survival status) value. We will have no means to compare to evaluate model accuracy. 

Although we may use the methods we have used in Chapter 7, where we use our model to predict on the train dataset and made a comparison with the original value to estimate the model's prediction accuracy. The similar method (OOB) is also used in the random forest models (in Chapter 8) to estimate the model's accuracy. We know that our estimated accuracy is not reliable. 

There is systematic method in data science used to evaluate a prediction model called "Cross Validation (CV)". This chapter we will demonstrate how to use CV to evaluate the models built in the precious two chapters.  
  
## Model's Underfitting and Overfitting

We have experienced that both of our decision tree models and random forest models have a problem that the model has a higher estimated accuracy and a much lower accuracy with the test dataset. This Would only mean two things with our prediction models ether is overfitting or underfitting.

let us quickly look at a very graphic example of underfitting, a good fit, and overfitting. 

```{r modelfit, fig.cap ="Model's fit by train and test data", out.width = "100%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "underfit.png"))

```

We can see that the first model is a straight line (a low variance model: $y$ = $m$ * $x$ + $c$) fails to capture the underlying parabolic curve in the data, this is underfitting. At the other extreme the high degree polynomial (a low bias model) captures too much of the noise at the same time as the underlying parabola, and is overfitting. Although it is following the data points provided (ie. the training dataset), this curve is not transferable to new data (ie. the test dataset).

Among the models we have produced, the decision tree model1 with only attribute *Sex* as its predictor is an example of underfitting model. It has 78.68% estimated accuracy on the training dataset but only has 76.56% accuracy on the test dataset. On the contrary, our random forest Model4 is seriously overfitted. It has  

## General Cross Validation Methods

There are two general Cross validation methods can be used to valid a prediction model:

1. Single model cross-validation
2. Multiple models comparison

### Single model Cross Validation
The goal of single model cross-validation is to test the model's ability to predict new data that was not used in model construction, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset or an unknown dataset.

One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.

There are two major cross validation methods: exhaustive Cross-validation and non-exhaustive Cross-validation. 

+ **Exhaustive cross-validation** learn and test on all possible ways to divide the original sample into a training and a validation set. **Leave-p-out cross-validation (LpO CV)** is an exhaustive cross validation method. It involves using $p$ data samples as the validation dataset and the remaining data samples as the training dataset. This is repeated over and over until all possible ways to divide the original data sample into a training and a validation dataset $p$. 

+ **Non-exhaustive cross validation**, in the contrary, does not compute all the possible ways of splitting the original data sample but still has a certain coverage. **$k$-fold cross-validation** is a typical non-exhaustive cross validation. In $k$-fold cross-validation, the original data sample is randomly partitioned into $k$ equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation dataset for testing the model, and the remaining $k$ − 1 subsamples are used as training data. The cross-validation process is then repeated $k$ times, with each of the $k$ subsamples used exactly once as the validation data. The $k$ results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used in practice. 

### General Procedure of Cross-validation

The general procedure of use of Cross validation is as follows:
1. Define cross validation folds $K$ and create training dataset and validation dataset by Shuffle the dataset randomly.
2. Specify parameters for Cross validation. 
3. Create model from training dataset
4. Create list of predicted values on validation dataset
5. Check prediction error and prediction accuracy 

We will use examples to demonstrate this procedure. 

### Cross Validation on Decision Tree Models

We have produced 4 decision tree models in Chapter 7. Let us do Cross validation on model3 and model4 since these two used re-Engineered dataset and perform badly on the test dataset.
```{r}
library(caret)
library(rpart)
library(rpart.plot)

#read Re-engineered dataset
RE_data <- read.csv("RE_data.csv", header = TRUE)

#Factorize response variable
RE_data$Survived <- factor(RE_data$Survived)
#Seperate Train and test data.
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]

#setup model's train and valid dataset
set.seed(1000)
samp <- sample(nrow(train), 0.7 * nrow(train))
trainData <- train[samp, ]
validData <- train[-samp, ]

###########################

# First, create cross validation folds, specifying parameters
set.seed(3214)
folds = createMultiFolds(trainData$Survived, 
                         k = 10, # number of folds
                         times = 5) # number of partitions
# Second, specify parameters for cross validation
control <- trainControl(method = "repeatedcv", 
                        index = folds,
                        number = 6, 
                        search = "grid")
#Third, create model from cross validation data

tree_model3_cv <- train(Survived ~ Pclass + Title + Sex + Age_group + Group_size + Ticket_class + Fare_pp + Deck + HasCabinNum + Embarked,  
                       data = trainData, 
                       method = "rpart", 
                       trControl = control)
```
```{r tree_model3_CV, out.width='32.8%', fig.show='hold', fig.cap='Decision Tree model3.'}
#Visualize cross validation tree

rpart.plot(tree_model3_cv$finalModel, extra=4)

print.train(tree_model3_cv)
plot.train(tree_model3_cv)

```

```

```{r }
#predict on train
predict_train <-predict(tree_model3_cv, train)
conMat <- confusionMatrix(predict_train, trainData$Survived)
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(tree_model3_cv, validData)
conMat <- confusionMatrix(predict_valid, validData$Survived)
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(tree_model3_cv, test)
submit3 <- data.frame(PassengerId = test$PassengerId, Survived = as.factor(predict_test))
write.csv(submit3, file = "Tree_model3_CV.CSV", row.names = FALSE)

## test accuracy 0.77751
```

```{r}
set.seed(1000)
cvCtrl <- trainControl(method= "cv", number = 10) # use 10-fold cross validation
Model4 <- train(as.factor(Survived) ~ Pclass+Title+Sex +Age_group+Group_size+Fare_pp,
#Model3 <- train(as.factor(Survived) ~  Pclass+Title+Sex,
                            data= train, method= "rpart",
                            trControl= cvCtrl,
                            tuneLength= 5)
print.train(Model4)
plot.train(Model4)

#predict on train
predict_train <-predict(Model4, train)
conMat <- confusionMatrix(as.factor(predict_train), as.factor(train$Survived))
conMat$table
#conMat$overall
predict_train_accuracy <- conMat$overall["Accuracy"]
predict_train_accuracy
#predict on valid
predict_valid <-predict(Model4, validData)
conMat <- confusionMatrix(as.factor(predict_valid), as.factor(validData$Survived))
conMat$table
#conMat$overall
predict_valid_accuracy <- conMat$overall["Accuracy"]
predict_valid_accuracy

#predict on test
predict_test <-predict(Model4, test)
submit4 <- data.frame(PassengerId = RE_data[892:1309,1], Survived = as.factor(predict_test))
write.csv(submit4, file = "Tree_ReEngineered_model4.CSV", row.names = FALSE)

## test accuracy 0.77751

```
```{r}
#mtry = 5* and *ntree = 1000*. The accuracy reaches: 81.78%.
set.seed(1111)
control <- trainControl(method="cv", number=10)

mtry <- 5
RF_random <- train(Survived ~., data=train, method="rf", metric=metric, tuneLength=5, trControl=control)
print(RF_random)
plot(RF_random)
```




Utilizing the split training data, let’s assess the accuracy of this model

First, create cross validation folds, specifying parameters

folds = createMultiFolds(train.cv$Survived, 
                         k = 10, # number of folds
                         times = 5) # number of partitions
Second, specify parameters for cross validation

control <- trainControl(method = "repeatedcv", 
                        index = folds,
                        number = 6, 
                        search = "grid")
Third, create model from cross validation data

# factorise
train.cv$Survived <- factor(train.cv$Survived)

tree.classifier.cv <- train(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare                             + Embarked + Title + Master.Male + Female.Group,  
                       data = train.cv, 
                       method = "rpart", 
                       trControl = control)
Visualise cross validation tree

rpart.plot(tree.classifier.cv$finalModel, extra=4)


Decision Tree’s are fairly intuitive to interpret. The far right leaf node (bottom of tree) can be interpreted as for non-survivors (blue) where sex == male, 81% perished. By contrast, for survivors (green) where sex != male (i.e. female) and cabin class == 1, 96% survived

Create list of predicted values

# Predicting the Validation set results
y_predDT.CV = predict(tree.classifier.cv, 
                      newdata = test.cv[,-which(names(test.cv) == "Survived")])
Create confusion matrix contrasting y actual versus y predicted values

# Checking the prediction accuracy
table(test.cv$Survived, y_predDT.CV) # Confusion matrix
##    y_predDT.CV
##      0  1
##   0 98 12
##   1 19 49
Create error estimate (where known survival status mismatches with predicted survival status)

# Check prediction accuracy 
error <- mean(test.cv$Survived != y_predDT.CV) # Misclassification error
Calculate Accuracy as 1 minus error

# 0.8258
paste('Accuracy =', round(1 - error, 4))
## [1] "Accuracy = 0.8258"
The result of cross validation is an accuracy estimate of 0.8258. Put another way, ~ 83% of passengers survival status was correctly predicted by the model

#decision tree model1 (accuracy 0.76555)
model1 <- rpart(Survived ~ Sex, data = train, method="class")
Prediction1 <- predict(model1, test, type = "class")

#decision tree model2 Accuracy 0.77511
model2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, method="class")

Prediction2 <- predict(model2, test, type = "class")

#decision tree model3 re-engineered dataset, Accuracy 0.75598
model3 <- rpart(Survived ~ Pclass+Title+Sex +Age_group+Group_size+Ticket_class+Fare_pp+Deck+HasCabinNum + Embarked,
              data=RE_data[1:891,],
              method="class")
Prediction3 <- predict(model3, RE_data[892:1309,], type = "class")

#decision tree model4 re-engineered dataset. Accuracy 0.75837
model4 <- rpart(Survived ~ Sex +  Fare_pp + Pclass,
              data=RE_data[1:891,],
              method="class")
Prediction4 <- predict(model4, RE_data[892:1309,], type = "class")

