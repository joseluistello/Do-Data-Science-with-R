# Titiannic Prediction with Random Forest

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,warning = FALSE)
```
```{r prediction1， out.width = "60%", fig.align ="center", echo =FALSE}
knitr::include_graphics(here::here("images", "forest-tree.jpg"))

```
>
> Can't see forest for the trees.
>
>                                     -- English Proverb
>

As we can see from the previous chapter, the decision tree models does not preform well in our prediction. Particularly, our re-engineered dataset performs worsen than the raw dataset. One reason could be the model itself. This chapter we will try different models to see if we can improve model's accuracy fby using **Random Forest** model. 

Random Forest model is one of the powerful ensembling machine learning algorithm which works by creating multiple decision trees and combining the output generated by each of the decision trees through a voting mechanism to produce the final output based on the majority of decision trees' votes.  The Figure \@ref(fig:forest) is an example of what a random forest classifier in general looks like:

```{r forest,  out.width = "50%", fig.align ="center", echo =FALSE, fig.cap="Example of the Random Forest."}
knitr::include_graphics(here::here("images", "Random_forest_diagram_complete.png"))
```

In random forest, the decision tree classifier does not select all the data samples and attributes in each of the trees. Instead, it randomly selects data samples and attributes in each of the tree that it creates and then combines the output at the end. It removes the bias that a decision tree model might introduce in the model. In random forest, multiple decision tree are created and based on the output of each model, a vote is carried out to find the result with the highest frequency. A test dataset is evaluated based on these outputs to get the final predicted results. Because of the evaluation process, the random forest model will have an estimated prediction accuracy once constructed. This models' estimated accuracy can be used to compare between random forest models.  

## Steps to Build a Random Forest

1. Randomly select $k$ attributes from total $m$ attributes where $k < m$, the default value of $k$ is generally $\sqrt{m}$.
2. Among the $k$ attributes, calculate the node $d$ using the **best split point**
3. Split the node into a number of nodes using the **best split method**. See Section \@ref(best_split), by default R random Forest uses Gini impurity values
4. Repeat the previous steps build an individual decision tree
5. Build a forest by repeating all steps for $n$ number times to create $n$ number of trees

After the random forest trees and classifiers are created, predictions can be made using the following steps:

1. Run the test data through the rules of each decision tree to predict the outcome and then 
2. Store that predicted target outcome
3. Calculate the votes for each of the predicted targets
4. Output the most highly voted predicted target as the final prediction 

Similar with the decision tree model, random forest also has many implementations already built. You do not need to write code to do the actual model construction. In R, you can use a package called  'randomForest'. There are a number of terminologies that are used in random forest algorithms need to be understood, such as:

1. **Variance**. When there is a change in the training data algorithm, this is the measure of that change. The most commonly used parameters to reflect changes are *ntree* and *mtry*. 

2. **Bagging**. This is a variance-reducing method that trains the model based on random sub-samples of training data. 

3. **Out-of-bag (oob)** error estimate - The random forest classifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training dataset. The out-of-bag (OOB) error is the average error for each calculation using predictions from the trees that do not contain their respective bootstrap sample. This enables the random forest classifier to be adjusted and validated during training. 

## Titanic prediciton with a Random Forest

Let’s now look at how we can implement the random forest algorithm for our Titanic prediction. 
R provides `'randomForest'` package. You can check the details of the package for full usage. We will start with direct function call with its default settings and we may change settings later. We will also use the original attributes first and then use re-engineered attributes to see if we can improve on the model.

### Random Forest with Key Predictors {-}
```{r echo = FALSE, warning=FALSE, message=FALSE}
# Install the random forest library, if you have not
# install.packages('randomForest')
# load library

library(randomForest)
library(plyr)
library(caret)
# load data if you have not

RE_data <- read.csv("RE_data.csv", header = TRUE)

# RE_data <- mutate_if(RE_data, is.numeric, as.factor)
# RE_data <- mutate_if(RE_data, is.factor, as.factor)
#                      
train <- RE_data[1:891, ]
test <- RE_data[892:1309, ]
```

The process of using `randomForest` package to build a RF model is same with the decision tree package "rpart".  Note also if a dependent (response) variable is a factor, classification is assumed, otherwise regression is assumed. So to uses randomForest, we need to convert dependent variable into factor. 

```{r}
# convert variables into factor
train$Survived <- as.factor(train$Survived)
# convert other attributes which really are categorical data but in form of numbers
train$Pclass <- as.factor(train$Pclass)
train$Group_size <- as.factor(train$Group_size)
#confirm types
sapply(train, class)
```
Let us use the same five most related attributes: `Pclass`,` Sex`, `HasCabinNum`, `Deck` and `Fare_pp` in the decision tree model2. We use all default parameters of the *randomForest*.

```{r}
# Build the random forest model uses pclass, sex, HasCabinNum, Deck and Fare_pp
set.seed(1234) #for reproduction 
RF_model1 <- randomForest(Survived ~ Sex + Pclass + HasCabinNum + Deck + Fare_pp, data=train, importance=TRUE)
```

Let us check model's prediction accuracy.
```{r }
RF_model1
```

We can see that the model uses default parameters: *ntree* = 500 and *mtry* = 1. The model's estimated accuracy is **80%**. It is 1 - 0.20 (OOB error).

Let us make a prediction on train dataset and check the accuracy. 

```{r}
# Make your prediction using the validate dataset
#set.seed(1234)
RF_prediction1 <- predict(RF_model1, train)
#check up
conMat<- confusionMatrix(RF_prediction1, train$Survived)
conMat$table
# Misclassification error
paste('Accuracy =', round(conMat$overall["Accuracy"],2))
paste('Error =', round(mean(train$Survived != RF_prediction1), 2)) 
```
We can see that prediction on train dataset has achieved **84%** accuracy. 
It has made 107 wrong prediction and 516 correct prediction on death. The prediction on survived is 33 wrong prediction out of 235 correct predictions. 

The model has an accuracy of 80% after learning, but our evaluation on the train dataset achieves 84%. It has been increased. Compare with the decision tree model2, which the the same attributes were used and the prediction accuracy on the train data was 81%, the accuracy is also increased. Let us make a prediction on test dataset and submit to Kaggle to obtain an accuracy score. 

```{r}
# produce a submit with Kaggle required format that is only two attributes: PassengerId and Survived
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

#make prediction
RF_prediction <- predict(RF_model1, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "RF_Result1.CSV", row.names = FALSE)
```
We can see our random forest model has scored **0.76555** by the Kaggle competition. It is interesting to know that the random forest model has not impoprved on the test dataset compare with the decision tree model with the same predictors. The accuracy was also 0.76555.

let us record these accuracies, 
```{r}
RF_model1_accuracy <- c(80, 84, 76.555)
```

### Random Forest with More Variables {-}

Now let us see if We can obtain a better model if we use more variables. The predictor we are using is identical to the decision tree model3. 
```{r}
set.seed(2222)
RF_model2 <- randomForest(Survived ~ Sex + Fare_pp + Pclass + Title + Age_group + Group_size + Ticket_class  + Embarked, data = train, importance=TRUE)
```
We can assess the new model,

```{r}
RF_model2
```

Notice that the default parameter *mtry = 2* and *ntree = 500*. It means the number of variable tried at each split is now 2 and number of trees can be built is 500. The model's estimated OOB error rate is 16.84%. It has a increase in comparison with the first model which was 20%. So the overall accuracy of the model has reached **83.16%**.

Let us make a prediction on train Data to verify the model's training accuracy.
```{r}
RF_prediction2 <- predict(RF_model2, train)
#check up
conMat<- confusionMatrix(RF_prediction2, train$Survived)
conMat$table
# Misclassification error
paste('Accuracy =', round(conMat$overall["Accuracy"],2))
paste('Error =', round(mean(train$Survived != RF_prediction2), 2)) 
```
We can see the accuracy on train dataset has reached 91%. The result shows that the prediction on survive has 55 wrong predictions out of 527 correct predictions; The prediction on death has 287 correct predictions and 22 wrong predictions. The overall accuracy reaches **91%**. It is again higher than the model learning accuracy **83.5%**.

It has also increased a bit comparing with the accuracy on the estimated accuracy **80%** and the accuracy on train dataset **84%** of the random forest RF_model1. Compare with the decision tree model3, which has the identical predictors, the accuracy was **85%** on the train dataset. 

Let us make another submit to Kaggle to see if the prediction on unseen data has been improved. 

```{r}
# produce a submit with Kaggle 
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

#make prediction
RF_prediction <- predict(RF_model2, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "RF_Result2.CSV", row.names = FALSE)
```
The feedback shows the prediction only increased a lot with a scored 0.78947! It has improved on the RF_model1 (0.76555) and decision tree model3 (0.77033).  

let us record these various accuracy.

```{r}
RF_model2_accuracy <- c(83.16, 92, 78.95)
```
### Random Forest with All Variables {-}

Now let use random forest to build a model with the maximum predictors that can be used from attributes. We may not be able to use all the attributes since the `randomForest` function cannot handle attribute which is not a factor and has over 53 levels. So, we will not use attribute `Ticket`. 
```{r}
set.seed(2233)
RF_model3 <- randomForest(Survived ~ Sex + Pclass + Age + SibSp + Parch + Embarked + HasCabinNum + Friend_size + Fare_pp + Title + Deck + Ticket_class + Family_size + Group_size + Age_group, data = train, importance=TRUE)
```
We can assess the new model,

```{r}
RF_model3
```
Notice that the default parameter *mtry* = 3 and ntree = 500. It means the number of variable tried at each split is now 3 and number of trees can be built is 500. The model's estimated OOB error rate is 17%. It has a increase in comparison with the model2 which was 18%. So the overall accuracy of the model has reached **83%**.

Let us make a prediction on train Data to verify the model's training accuracy.
```{r}
RF_prediction3 <- predict(RF_model3, train)
#check up
conMat<- confusionMatrix(RF_prediction3, train$Survived)
conMat$table
# Misclassification error
paste('Accuracy =', round(conMat$overall["Accuracy"],2))
paste('Error =', round(mean(train$Survived != RF_prediction3), 2)) 
```
We can see the accuracy on train dataset has reached 95%. The result shows that the prediction on survive has 38 wrong predictions out of 536 correct predictions; The prediction on death has 304 correct predictions and 13 wrong predictions. The overall accuracy reaches **95%**. It is again higher than the model learning accuracy **83%**.

Let us make another submit to Kaggle to see if the prediction on unseen data has been improved. 

```{r}
# produce a submit with Kaggle 
test$Pclass <- as.factor(test$Pclass)
test$Group_size <- as.factor(test$Group_size)

#make prediction
RF_prediction <- predict(RF_model3, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = RF_prediction)
# Write it into a file "RF_Result.CSV"
write.csv(submit, file = "RF_Result3.CSV", row.names = FALSE)
```
The feedback score is 0.77033. It shows decrease of the accuracy. 

let us record these various accuracy.

```{r}
RF_model3_accuracy <- c(83, 94, 77)
```

### Comparision the Three Random Forest Models

We have produced three random forest models, each has different performance in terms of prediction accuracy on the test dataset. Let us make a quick comparison among them.

```{r RF-model}
library(tidyr)
Model <- c("RF_Model1","RF_Model2","RF_Model3")
Pre <- c("Sex, Pclass, HasCabinNum, Deck, Fare_pp", "Sex, Fare_pp, Pclass, Title, Age_group, Group_size, Ticket_class, Embarked", "Sex, Pclass, Age, SibSp, Parch, Embarked, HasCabinNum, Friend_size, Fare_pp, Title, Deck, Ticket_class, Family_size, Group_size, Age_group")

Learn <- c(80.0, 83.16, 83.0)
Train <- c(84, 92, 78)
Test <- c(76.555, 78.95, 77.03)
df1 <- data.frame(Model, Pre, Learn, Train, Test)
df2 <- data.frame(Model, Learn, Train, Test)
knitr::kable(df1, longtable = TRUE, booktabs = TRUE, digits = 2, col.names =c("Models", "Predictors", "Accuracy on Learn", "Accuracy on Train", "Accuracy on Test"), 
  caption = 'The Comparision among 3 Random Forest models'
)
```
```{r RFmodelcompare, fig.cap = "Random Froest models' accuracy on model learning, Train dataset and Test dataset."}
df.long <- gather(df2, Dataset, Accuracy, -Model, factor_key =TRUE)
ggplot(data = df.long, aes(x = Model, y = Accuracy, fill = Dataset)) +
  geom_col(position = position_dodge()) 

```
From the comparison, we can see that:

1. It is not true that the more predictors the better performance with Random Forest models.
2. The result of the model validation on the training dataset is not reliable. The higher accuracy on the train dataset does not mean a higher accuracy on the test dataset. 
3. All the model has a degree of overfitting. That is the accuracy on the test data is lower than the train dataset and even lower than the model its own estimated accuracy while learn or construct it.
4. the cause of the overfitting is complicated issue. It may related with all the factors: number of predictors used to build the model, the dataset used to build the model and the model default parameters.

In comparison with the decision tree models we have built in the previous Chapter. The random forest models over performs all the four models on the test dataset. The lowest accuracy is the same with the highest accuracy with the decision tree models (76.55%).  

## Summary

In this Chapter we have demonstrated the use of random forest models for the Titanic problem. We have tried using different numbers of the predictors. Their accuracy on the test dataset has been illustrated in the figure \@ref(fig:RFmodelcompare).

Despite the efforts in features' engineering, the careful selection of the predictors, the random forest models have higher accuracy on the train dataset but fall dramatically with the test dataset. It demonstrated the practical problem in data science project that is overfitting. Overfitting is a serious problem because it is the test dataset (unseen data) matters in real practice. Overfitting can be discovered and eliminated with Cross validation that is waht we are going to discuss in the next Chapter.  


## Excercise 8

1. Find out what is "OOB estimate of error rate"? How to reduce its value?

2. In a random forest model, its Confusion matrix shows misclassified samples and their error rate. Explain the concept of the "Positive error" and "Negative error" how to balance them? 

3. Try different sampling methods by using different fold and repeat numbers in the Cross Validation to see the affect of the tune parameters and model's accuracy.

4. Explore train method in caret with different models and methods
   

## References

Trevor Hastie, Rob Tibshirani, Jerome Friedman (2009) “Statistical Learning”
(Springer).

Understanding random forests with randomForestExplainer
Aleksandra Paluszyńska https://cran.r-project.org/web/packages/randomForestExplainer/vignettes/randomForestExplainer.html



